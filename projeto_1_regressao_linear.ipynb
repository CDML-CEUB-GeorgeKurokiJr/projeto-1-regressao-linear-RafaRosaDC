{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 0. IMPORTANDO AS BIBLIOTECAS NECESSÁRIAS\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n"
      ],
      "metadata": {
        "id": "CCitSGUB7XRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CARREGAMENTO\n",
        "\n",
        "df = pd.read_csv('hour.csv')\n"
      ],
      "metadata": {
        "id": "edlQgepR7cgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 PRÉ-VISUALIZAÇÃO E ANÁLISE EXPLORATÓRIA (EDA)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFORMAÇÕES GERAIS DO DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dimensão\n",
        "print(f\"Dimensão do dataset: {df.shape[0]} linhas x {df.shape[1]} colunas\\n\")\n",
        "\n",
        "# Primeiras linhas\n",
        "print(\"Primeiras 5 linhas:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# Tipos de dados\n",
        "print(\"Tipos de variáveis:\")\n",
        "print(df.dtypes, \"\\n\")\n",
        "\n",
        "# Valores ausentes\n",
        "print(\"Valores ausentes por coluna:\")\n",
        "print(df.isnull().sum(), \"\\n\")\n",
        "\n",
        "# Estatísticas descritivas\n",
        "print(\"Estatísticas descritivas:\")\n",
        "print(df.describe(), \"\\n\")\n",
        "\n",
        "\n",
        "# Correlação com variável alvo\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CORRELAÇÃO COM A VARIÁVEL ALVO (cnt)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "correlation = df.corr(numeric_only=True)['cnt'].sort_values(ascending=False)\n",
        "print(correlation)\n",
        "\n",
        "\n",
        "# VISUALIZAÇÕES EXPLORATÓRIAS\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Distribuição do target\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(df['cnt'], bins=50, kde=True)\n",
        "plt.title(\"Distribuição do Total de Aluguéis (cnt)\")\n",
        "plt.xlabel(\"Quantidade de Bicicletas\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "\n",
        "# Média por hora\n",
        "plt.subplot(1, 3, 2)\n",
        "hourly_avg = df.groupby('hr')['cnt'].mean()\n",
        "sns.lineplot(x=hourly_avg.index, y=hourly_avg.values)\n",
        "plt.title(\"Média de Aluguéis por Hora\")\n",
        "plt.xlabel(\"Hora do Dia\")\n",
        "plt.ylabel(\"Média de Bicicletas\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Heatmap de correlação\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.heatmap(df.corr(numeric_only=True), cmap='coolwarm', center=0)\n",
        "plt.title(\"Mapa de Correlação\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INSIGHTS RÁPIDOS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "• A variável 'cnt' representa o total de aluguéis por hora.\n",
        "• Espera-se forte correlação com:\n",
        "    - registered\n",
        "    - temp / atemp\n",
        "    - hr (efeito horário)\n",
        "• Variáveis climáticas influenciam significativamente o volume.\n",
        "• Distribuição de 'cnt' geralmente apresenta assimetria positiva.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "KI4qAPRE7fC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. FEATURE ENGINEERING\n",
        "\n",
        "# Codificação Cíclica da Hora\n",
        "df['hr_sin'] = np.sin(2 * np.pi * df['hr'] / 24)\n",
        "df['hr_cos'] = np.cos(2 * np.pi * df['hr'] / 24)\n",
        "\n",
        "# Interações importantes\n",
        "df['temp_hr'] = df['temp'] * df['hr_sin']\n",
        "df['humidity_temp'] = df['hum'] * df['temp']\n",
        "\n",
        "# Target\n",
        "y = df['cnt'].values.reshape(-1, 1)\n",
        "\n",
        "# Removendo colunas que causam data leakage\n",
        "X_raw = df.drop(columns=['cnt', 'casual', 'registered', 'instant', 'dteday', 'hr'])\n",
        "\n",
        "# One-hot nas categóricas (sem hr agora)\n",
        "cat_cols = ['season', 'mnth', 'weekday', 'weathersit']\n",
        "X_encoded = pd.get_dummies(X_raw, columns=cat_cols, drop_first=True)\n",
        "\n",
        "X = X_encoded.values\n"
      ],
      "metadata": {
        "id": "IRh8kZlB7jJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. SPLIT\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "8xOlRkRS7owI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ESCALONAMENTO\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n"
      ],
      "metadata": {
        "id": "Yy6cGZux7uzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. TENSORES\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "XDlw75ka70TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MODELO MLP REGULARIZADO\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "model = MLPRegressor(input_dim)\n",
        "\n",
        "criterion = nn.HuberLoss(delta=1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "131_f6Ly75Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. TREINAMENTO COM EARLY STOPPING\n",
        "\n",
        "epochs = 1000\n",
        "patience = 50\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # ---- Treino ----\n",
        "    model.train()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # ---- Validação ----\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(X_test_tensor)\n",
        "        val_loss = criterion(val_pred, y_test_tensor)\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(f\"Early stopping ativado na época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "# Carrega melhor modelo\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n"
      ],
      "metadata": {
        "id": "TEjc3Odc77v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. AVALIAÇÃO FINAL\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions_scaled = model(X_test_tensor)\n",
        "\n",
        "    predictions_real = scaler_y.inverse_transform(predictions_scaled.numpy())\n",
        "    y_test_real = scaler_y.inverse_transform(y_test_tensor.numpy())\n",
        "\n",
        "    mse = np.mean((predictions_real - y_test_real) ** 2)\n",
        "    mae = np.mean(np.abs(predictions_real - y_test_real))\n",
        "    r2 = r2_score(y_test_real, predictions_real)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTADOS FINAIS OTIMIZADOS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"MAE : {mae:.0f} bicicletas/hora\")\n",
        "print(f\"MSE : {mse:.0f}\")\n",
        "print(f\"R²  : {r2:.4f} ({r2*100:.2f}%)\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "bawOoMvX8Czz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. GRÁFICOS\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Curva de Loss\n",
        "axes[0].plot(train_losses, label='Train Loss')\n",
        "axes[0].plot(val_losses, label='Validation Loss')\n",
        "axes[0].set_title(\"Curva de Treinamento\")\n",
        "axes[0].legend()\n",
        "axes[0].grid()\n",
        "\n",
        "# Real vs Predito\n",
        "axes[1].scatter(y_test_real, predictions_real, alpha=0.3)\n",
        "min_val = min(np.min(y_test_real), np.min(predictions_real))\n",
        "max_val = max(np.max(y_test_real), np.max(predictions_real))\n",
        "axes[1].plot([min_val, max_val], [min_val, max_val], color='red')\n",
        "axes[1].set_title(\"Real vs Predito\")\n",
        "axes[1].grid()\n",
        "\n",
        "# Resíduos\n",
        "residuos = y_test_real - predictions_real\n",
        "sns.histplot(residuos, bins=50, kde=True, ax=axes[2])\n",
        "axes[2].set_title(\"Distribuição dos Resíduos\")\n",
        "axes[2].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uMuqcrLa8ICU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}